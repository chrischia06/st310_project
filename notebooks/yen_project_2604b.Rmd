---
title: "Project"
author: "Zhen-Yen Chan"
date: "4/21/2021"
#output: pdf_document
output: html_notebook
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(ggplot2)
library(arm)
library(car)
library(tidyr)
library(glmnet)
library(caret)
library(tidymodels)
library(pROC)
library(randomForest)
library(e1071)
```

## Load data, set seed
```{r}
df <- read.csv(file = "train.csv")
set.seed(1)
```

## Pre-processing
```{r}
rownames(df) = df$id
df = df[,-1] # remove id column
df$target=as.factor(df$target)
#head(df)
#summary(df)
```

```{r}
# turn all categorical variables into factors
df$cat1 <- as.factor(df$cat1)
df$cat2 <- as.factor(df$cat2)
df$cat3 <- as.factor(df$cat3)
df$cat4 <- as.factor(df$cat4)
df$cat5 <- as.factor(df$cat5)
df$cat6 <- as.factor(df$cat6)
df$cat7 <- as.factor(df$cat7)
df$cat8 <- as.factor(df$cat8)
df$cat9 <- as.factor(df$cat9)
df$cat10 <- as.factor(df$cat10)
df$cat11 <- as.factor(df$cat11)
df$cat12 <- as.factor(df$cat12)
df$cat13 <- as.factor(df$cat13)
df$cat14 <- as.factor(df$cat14)
df$cat15 <- as.factor(df$cat15)
df$cat16 <- as.factor(df$cat16)
df$cat17 <- as.factor(df$cat17)
df$cat18 <- as.factor(df$cat18)
```

## Exploratory Data Analysis

Plot continuous variables in boxplots
```{r}
# colnames(df)
# stack variables to plot
cont.stacked <- gather(data=df[,20:31],-target,key="var",value="value")
p.cont <- ggplot(cont.stacked,aes(x=target,y=value),fill=factor(value)) + geom_boxplot() + coord_flip() + facet_wrap(~var, scales="free_x")
p.cont
```

Plot categorical variables in barplots
```{r}
# divide categorical variables into 2 groups
g1 <- c(1:10,31)
g2 <- c(11:19,31)
cate.stacked1 <- gather(data=df[,g1],-target,key="var",value="value")
p.cate1 <- ggplot(cate.stacked1,aes(x=value,fill=target)) + geom_bar(position="fill") + scale_y_continuous(name = "Within group Percentage", labels = scales::percent) + facet_wrap(~var, scales="free_x")
cate.stacked2 <- gather(data=df[,g2],-target,key="var",value="value")
p.cate2 <- ggplot(cate.stacked2,aes(x=value,fill=target)) + geom_bar(position="fill") + scale_y_continuous(name = "Within group Percentage", labels = scales::percent) + facet_wrap(~var, scales="free_x")
p.cate1
p.cate2
```

## Split and prepare data for modelling
```{r}
sam = sample(1:nrow(df), 4000)
df_sample = df[sam,]
df_split <- initial_split(df_sample, prop = 3/4)
df_train <- training(df_split)
df_test <- testing(df_split)
```

## Model 1 (baseline model): logistic regression with few predictors 

1) Run logistic regression with a few variables (selected visually from plots)
```{r}
glm1 <- glm(target~cont3+cont4+cat13+cat18,data=df_train, family=binomial(link="logit"),control = list(maxit = 100))
summary(glm1)
```

```{r}
preds1 <- predict(glm1, df_test, type="response")
pred_classes1 <- ifelse(preds1 > 0.5, 1,0)
acc1=mean(pred_classes1 == df_test$target)
roc1=roc(df_test$target, pred_classes1)
auc1=auc(roc1)
cbind(acc1,auc1)
```


3.1) Run logistic regression for all variables
```{r}
glm2 <- glm(target~.-cont1-cont2-cont8,data=df_train, family=binomial(link="logit"),control = list(maxit = 100))
library(arm)
augment(glm2)
#display(glm2)
```

Make both train and test sets have the same levels
```{r}
glm2$xlevels[["cat1"]] <- union(glm2$xlevels[["cat1"]], levels(df_test$cat1))
glm2$xlevels[["cat2"]] <- union(glm2$xlevels[["cat2"]], levels(df_test$cat2))
glm2$xlevels[["cat3"]] <- union(glm2$xlevels[["cat3"]], levels(df_test$cat3))
glm2$xlevels[["cat4"]] <- union(glm2$xlevels[["cat4"]], levels(df_test$cat4))
glm2$xlevels[["cat5"]] <- union(glm2$xlevels[["cat5"]], levels(df_test$cat5))
glm2$xlevels[["cat6"]] <- union(glm2$xlevels[["cat6"]], levels(df_test$cat6))
glm2$xlevels[["cat7"]] <- union(glm2$xlevels[["cat7"]], levels(df_test$cat7))
glm2$xlevels[["cat8"]] <- union(glm2$xlevels[["cat8"]], levels(df_test$cat8))
glm2$xlevels[["cat9"]] <- union(glm2$xlevels[["cat9"]], levels(df_test$cat9))
glm2$xlevels[["cat10"]] <- union(glm2$xlevels[["cat10"]], levels(df_test$cat10))
glm2$xlevels[["cat11"]] <- union(glm2$xlevels[["cat11"]], levels(df_test$cat11))
glm2$xlevels[["cat12"]] <- union(glm2$xlevels[["cat12"]], levels(df_test$cat12))
glm2$xlevels[["cat13"]] <- union(glm2$xlevels[["cat13"]], levels(df_test$cat13))
glm2$xlevels[["cat14"]] <- union(glm2$xlevels[["cat14"]], levels(df_test$cat14))
glm2$xlevels[["cat15"]] <- union(glm2$xlevels[["cat15"]], levels(df_test$cat15))
glm2$xlevels[["cat16"]] <- union(glm2$xlevels[["cat16"]], levels(df_test$cat16))
glm2$xlevels[["cat17"]] <- union(glm2$xlevels[["cat17"]], levels(df_test$cat17))
glm2$xlevels[["cat18"]] <- union(glm2$xlevels[["cat18"]], levels(df_test$cat18))
```


Calculate test accuracy and AUC 
```{r}
preds2 <- predict(glm2, df_test, type="response")
pred_classes2 <- ifelse(preds2 > 0.5, 1,0)
acc2=mean(pred_classes2 == df_test$target)
roc2=roc(df_test$target, pred_classes2)
auc2=auc(roc2)
cbind(acc2,auc2)
```

```{r}
#library(GGally)
ggpairs(df[,c(1:5,7,10)], cardinality_threshold=20)
ggpairs(df[,12:19], cardinality_threshold=20)
```

cat5 has 84 levels
cat7 has 51 levels
cat8 has 61 levels
cat10 has 299 levels

// Identify overall significant variables
```{r}
library(car)
#Anova(glm0)
```

// Check for multicollinearity from multiple linear regression
```{r}
mlr1 <- lm(target~., data=df_train)
library(car)
vif(mlr1)
attributes(alias(mlr1)$Complete)$dimnames[[1]]
mlr1 <- lm(target~.-cat8-cat10, data=df_train)
vif(mlr1)
```


Using tidymodels
```{r}
logit_fit <- 
  logistic_reg(mode = "classification") %>%
  set_engine(engine = "glmnet", trace.it =TRUE, family='binomial') %>% 
  fit(target ~ cat0+cat1+cat2+cat7+cat11+cat14+cat15+cat16+cat18+cont5+cont8+cont10, data = df_train)

model_res <- 
  logit_fit %>% 
  pluck("fit") %>% 
  summary()
model_res
```

Test accuracy and AUC
// pred_classes has 2 classes
```{r}
preds1 <- predict(logit_fit, df_test, penalty = 0.0007, type="prob")
pred_classes1 <- ifelse(preds1 > 0.5, 1,0)
mean(pred_classes1 == df_test$target)
roc1=roc(df_test$target, pred_classes1)
auc(roc1)
```

Baseline test accuracy: 73.5%

One-hot encode categorical variables
```{r}
X_train = df_train[, -length(df_train)]
y_train <- df_train$target
# dmy1 <- dummyVars("~.", data=X_train)
# X_train <- data.frame(predict(dmy1, newdata=X_train))
# df_train=cbind(X_train, y_train)
# colnames(df_train)[colnames(df_train) == "y_train"] <- "target" # Rename column

X_test = df_test[, -length(df_test)]
y_test <- df_test$target
# dmy2 <- dummyVars("~.", data=X_test)
# X_test <- data.frame(predict(dmy2, newdata=X_test))
# df_test=cbind(X_test, y_test)
# colnames(df_test)[colnames(df_test) == "y_test"] <- "target" # Rename column
```

## Logistic ridge regression

Put data into matrices
```{r}
X_train = model.matrix(~., X_train)
X_test = model.matrix(~., X_test)
```


```{r}
glm3 <- cv.glmnet(X_train, y_train, family="binomial"(link="logit"), alpha=0)
glm3
```

Test accuracy and ROC
```{r}
preds3=predict(glm3, newx=X_test, type="response", s=glm3$lambda.min)
pred_classes3 <- ifelse(preds3 > 0.5, 1,0)
acc3=mean(pred_classes3==y_test)
roc3=roc(y_test, pred_classes3)
auc3=auc(roc3)
cbind(acc3,auc3)
```

## Random forest

Define the control 
```{r}
control <- trainControl(method = "cv",
    number = 10,
    search = "grid")
```

4a) Random forest with default parameters
```{r}
# library(caret)
ptm=proc.time()
set.seed(1)
rf1 <- train(target~.,
    data = df_train,
    method = "rf",
    metric = "Accuracy",
    trControl=control, 
    importance=T,
    ntree=64)
time1=proc.time()-ptm
time1
#ls(rf1)
varImpPlot(rf1$finalModel, n.var=30, type=1)
```

```{r}
preds_rf1 <- predict(rf1, df_test, type="prob")
pred_classes_rf1 <- ifelse(preds_rf1 > 0.5, 1,0)
acc_rf1=mean(pred_classes_rf1[,2] == df_test$target)
roc_rf1=roc(df_test$target, pred_classes_rf1[,2])
auc_rf1=auc(roc_rf1)
cbind(acc_rf1,auc_rf1)
```


top 19 variables: cat1,4,7,8,9,11,14,15,16,17,18, cont0,2,4,5,6,8,9,10

4b) random forest with importance variables (reduced model)
```{r}
ptm=proc.time()
set.seed(1)
rf2 <- train(target~cat1+cat4+cat7+cat8+cat9+cat11+cat14+cat15+cat16+cat17+cat18+
               cont0+cont2+cont4+cont5+cont6+cont8+cont9+cont10,
    data = df_train,
    method = "rf",
    metric = "Accuracy",
    trControl=control, 
    importance=T,
    ntree=8)
time2=proc.time()-ptm
time2
```

```{r}
preds_rf2 <- predict(rf2, df_test, type="prob")
pred_classes_rf2 <- ifelse(preds_rf2 > 0.5, 1,0)
acc_rf2=mean(pred_classes_rf2[,2] == df_test$target)
roc_rf2=roc(df_test$target, pred_classes_rf2[,2])
auc_rf2=auc(roc_rf2)
cbind(acc_rf2,auc_rf2)
```

## Tuning random forest

Search for best mtry
```{r}
tunegrid <- expand.grid(mtry = c(1: 10))
ptm=proc.time()
rf_mtry <- train(target~.,
    data = df_train,
    method = "rf",
    metric = "Accuracy",
    tuneGrid = tunegrid,
    trControl = control,
    importance = TRUE,
    ntree = 8)
rf_mtry
plot(rf_mtry)
time2=proc.time()-ptm
time2
best_mtry=max(rf_mtry$bestTune$mtry)
best_mtry
```

Search for best maxnodes
```{r}
store_maxnode <- list()
tunegrid <- expand.grid(.mtry = best_mtry)
ptm=proc.time()
for (maxnodes in c(20: 30)) {
    rf_maxnode <- train(target~.,
        data = df_train,
        method = "rf",
        metric = "Accuracy",
        tuneGrid = tunegrid,
        trControl = control,
        importance = TRUE,
        nodesize = 14,
        maxnodes = maxnodes,
        ntree = 8)
    iter <- toString(maxnodes)
    store_maxnode[[iter]] <- rf_maxnode
}
results_node <- resamples(store_maxnode)
summary(results_node)
time3=proc.time()-ptm
time3
best_maxnodes=23
```

Search for best ntree
```{r}
store_maxtrees <- list()
ptm=proc.time()
for (ntree in c(250, 300, 350, 400, 450, 500, 550, 600, 800, 1000, 2000)) {
    rf_maxtrees <- train(target~.,
        data = df_train,
        method = "rf",
        metric = "Accuracy",
        tuneGrid = tunegrid,
        trControl = control,
        importance = TRUE,
        nodesize = 14,
        maxnodes = 23,
        ntree = ntree)
    key <- toString(ntree)
    store_maxtrees[[key]] <- rf_maxtrees
}
results_tree <- resamples(store_maxtrees)
summary(results_tree)
time4=proc.time()-ptm
time4
#best_ntree=24
```


References:
https://www.guru99.com/r-random-forest-tutorial.html
