---
title: "ST310 Course Project"
author: "Chris Chia, Mun Fai Chan, Zhen Yen Chan"
date: "`r format(Sys.time(), '%d/%m/%y')`"
output:
  pdf_document: default
  html_notebook: default
  html_document:
    df_print: paged
header-includes:
    - \usepackage {hyperref}
    - \hypersetup {colorlinks = true, linkcolor = blue, urlcolor = blue}
references:
- id: hastie2009elements
  title: "The Elements of Statistical Learning: Data Mining, Inference, and Prediction"
  author: "Hastie, T. and Tibshirani, R. and Friedman, J.H."
  publisher: Springer
  series: Springer series in statistics
  type: book
  issued:
    year: 2009
---

## ST310 Course Project


## Exploratory Data Analysis

We have obtained data from [](), 

```{r chunk}
library(tidyverse)
library(ggplot2)
df <- read.csv(file = "../data/train.csv")
cat(c("The dimensions of the array are :", dim(df)[1], ", ", dim(df)[2]))
```

The dataset consists of 30 features,  19 *categorical variables* and 11 numerical variables. We will need to process these categorical variables in some manner, which we shall consider in the [subequent section](#).

With 30 variables, and no *specific* knowledge about which features may be potentially useful.

### Univariate 

```{r}
rownames(df) = df$id
df = df[,-1]
X = df[, -length(df)]
y <- df$target
```

```{r}
df %>% pivot_longer(cols = starts_with("cont"), names_to  = "cont") %>% 
   ggplot(aes(x = value))+
   geom_histogram(bins = 100, alpha = 0.85)+
   ggtitle("Continuous features distribution")+
   facet_wrap(cont~.,scales = "free")+
   theme_minimal()
```


```{r}
df %>% pivot_longer(cols = contains(c("cat")), names_to  = "cat") %>% 
   ggplot(aes(x = value))+
   geom_bar(alpha = 0.85)+
   ggtitle("Categorical features distribution")+
   facet_wrap(cat~.,scales = "free")+
   theme_minimal()
```
### Bivariate 

```{r}
library(plotly)
df_num <- select_if(df[,-1], is.numeric)
cor_matrix <- cor(df_num)
library(reshape2)
cor_matrix <- melt(cor_matrix)
# plt <- heatmap()
# ggplotly(plt)
plt <- ggplot(data = cor_matrix, aes(x=Var1, y=Var2, fill=value)) + 
  geom_tile() + scale_fill_gradient(name="Pearson Correlation")
ggplotly(plt)
```
```{r}
pcs <- prcomp(df_num[,-length(df_num)])
set.seed(2021)
ind <- sample(1:dim(df)[1], 20000)
sample <- data.frame(pcs$x[ind,1], pcs$x[ind,2], df[ind, "target"])
names(sample) = c("pc1", "pc2", "y")
ggplot(sample) + geom_jitter(aes(x = pc1, y = pc2, colour = factor(y)),alpha=0.7) + ggtitle('Principal Components')
```

## Modelling

Linear Fit (Logistic Regression)
```{r}
fit <- lm(x= df$cont0, y=df$target)
```

Nonlinear Fit (Tree-based)
```{r}
library(tidymodels)
```

## Interpretation

